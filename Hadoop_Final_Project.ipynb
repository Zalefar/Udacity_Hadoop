{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Hadoop Final Project \n",
    "###Udacity Course: Intro to Hadoop and MapReduce  \n",
    "####Zach Farmer\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Table of Contents     \n",
    "[Introduction](#Introduction)      \n",
    "[Decision Process](#Decision Process)    \n",
    "[Student Times](#Student Times)    \n",
    "[Post and Answer Length](#Post and Answer Length)   \n",
    "[Top Tags](#Top Tags)    \n",
    "[Study Groups](#Study Groups)   \n",
    "[Search Functionality](#Search Functionality)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Introduction\"></a> \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This notebook contains the code for the final project but will not be able to run on a native Hadoop Distributed File System (HDFS) from this notebook. The code is designed to utilize Cloudera's Hadoop framework and was tested and run on a local (my machine) VM containing the necessary software. However I will use the Unix bourne shell as a proxy to approximate the application seen in the hadoop environment. It should be noted that this approximation happens on just one machine and cannot scale the same way as an actual HDFS environment it is only an approximation and shares little in common with an actual HDFS distribtuion. The mappers and reducers should however be portable as written in this report and could be implemented in an HDFS framework as is. The Cloudera framework supports Hadoop streaming and as a result the mappers and reducers are written in python. Their are two datasets, one which is a small sample and was used to confirm that the process and code is valid and a final dataset for which the mapreduce task was designed to be run on.  \n",
    "\n",
    "###Introduction\n",
    "*Intro to Hadoop and MapReduce project\n",
    "In this project you will work with some discussion forum (also sometimes called discussion board) data. It is a type of user generated content that you can find all around the web. Most popular websites have some kind of a forum, and the things you will do in this project can transfer to other similar projects. This page will be followed by various questions about the data set.* __Udacity -- Intro to Hadoop and MapReduce Final Project__     \n",
    "\n",
    "**The Data Set**   \n",
    "This particular dataset was taken from the Udacity forums the first months after the launch of this course. Udacity forums were run on a free, opensource software called OSQA, which was designed to be similar to the popular StackOverflow forums. The basic structure is - the forum has nodes. All nodes have a body and author_id. Top level nodes are called questions, and will also have a title and tags. Questions can have answers. Both questions and answers can have comments.\n",
    "\n",
    "You will have to run the code mostly on your VMs, or on your real Hadoop cluster, if you have set up one. You can download the additional dataset [here](http://content.udacity-data.com/course/hadoop/forum_data.tar.gz \"http://content.udacity-data.com/course/hadoop/forum_data.tar.gz\"). To unarchive it, download it to your VM, put in the data directory and run:  \n",
    "\n",
    "`tar zxvf forum_data.tar.gz`     \n",
    "\n",
    "There are 2 files in the dataset. The first is \"forum_nodes.tsv\", and that contains all forum questions and answers in one table. It was exported from the RDBMS by using tab as a separator, and enclosing all fields in doublequotes. If you finished Lesson 4, you already know how to deal with such files. You can find the field names in the first line of the file \"forum_node.tsv\". The ones that are the most relevant to the task are:      \n",
    "\n",
    "* \"id\": id of the node      \n",
    "* \"title\": title of the node. in case \"node_type\" is \"answer\" or \"comment\", this field will be empty        \n",
    "* \"tagnames\": space separated list of tags     \n",
    "* \"author_id\": id of the author      \n",
    "* \"body\": content of the post     \n",
    "* \"node_type\": type of the node, either \"question\", \"answer\" or \"comment\"      \n",
    "* \"parent_id\": node under which the post is located, will be empty for \"questions\"     \n",
    "* \"abs_parent_id\": top node where the post is located      \n",
    "* \"added_at\": date added     \n",
    "\n",
    "The second table is \"forum_users.tsv\". It contains fields for \"user_ptr_id\" - the id of the user. \"reputation\" - the reputation, or karma of the user, earned when other users upvote their posts, and the number of \"gold\", \"silver\" and \"bronze\" badges earned. The actual database has more fields in this table, like user name nickname, bio (if set) etc, but we have removed this information here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#unzip the tar.gz file if forum_data is not already in the local directory.\n",
    "import os \n",
    "\n",
    "#print os.listdir(os.getcwd())\n",
    "if \"forum_data\" not in os.listdir(os.getcwd()):\n",
    "    !tar zxvf forum_data.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Decision Process\"></a>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Decision Process       \n",
    "***Let's assume you have an active community site, similar to the Udacity forum, where users can post different information. You want to obtain some statistics about user behavior. Is it a a good idea to use MapReduce/Hadoop to process the data? Consider how each of the 3V's of Big Data would affect this decision process.***    \n",
    "\n",
    "**Answer:**    \n",
    "1. Volume     \n",
    "2. Variety    \n",
    "3. Velocity      \n",
    "\n",
    "Udacity may have somewhere in the neighborhood of several million users and it is very likely that something less then a significant number of those users  have utilized the forums. Storage prices continue to fall and most of the forum data is in a text format and possibly could fit on just a few large storage drives. Certainly the data for this project easily fits on one machine. Therefore on the basis of the V related to Volume you could make a case that a Hadoop process may not be that necessary. This assessment depends on how much collective data accrues over the lifespan of the forum, if this data were in excess of several terabytes then perhaps an HDFS would make sense in order to process the data. \n",
    "As was stated in the prompt many users provide very different types of posts and information; text, code snippets, numbers, images (pngs,etc.), html and latex for example. A large Variety of data may be recored in the forums and it is true that implementing a HDFS would simplify the storage and extend the range of acceptable inputs for forum posts by accepting all data in it's raw format. This flexibility could be especially convenient if Udacity decides that it wants to collect more data or except a wider range of data types from the forums and doesn't want to have to overhaul it's RDMS schemas to accommodate changing strategies and maturing data analytics.       \n",
    "Finally in regards to Velocity of data you could make that case that most forum data remains relevant for long periods of time and it doesn't appear to be a good strategy to dump old posts owing to storage limitations, rather acquring more space would be the preferred approach. The speed in which the data comes and is processed into usable statistics is likely not that important. The stastics to be run on the forum data probably do not have a super time sensitive nature. A case can be made therefore that an HDFS may not be necessary on the basis of data velocity, assuming of course that statistics are not being run on a very large amount of data that could not fit on a single machine in which case we would want to implement the statistics over and HDFS if only to more quickly debug our code.        \n",
    "\n",
    "Overall we can conclude that using a MapReduce/Hadoop to process the data would be a viable design decision (a good idea) as a consequence of the large variety of data found in the forum post and the possibly large quantities of forum posts which over time will continue to grow. Velocity of the data is not as significant in our decison process to utilize HDFS in this instance.    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Student Times\"></a>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Student Times   \n",
    "We have a lot of passionate students that bring a lot of value to forums. Forums also sometimes need a watchful eye on them, to make sure that posts are tagged in a way that helps to find them, that the tone on forums stays positive, and in general - they need people who can perform some management tasks - forum moderators. These are usually chosen from students who already have shown that they are active and helpful forum participants.      \n",
    "\n",
    "Our students come from all around the world, so we need to know both at what times of day the activity is the highest, and to know which of the students are active at that time.     \n",
    "\n",
    "In this exercise your task is to find for each student what is the hour during which the student has posted the most posts. Output from reducers should be:    \n",
    "   \n",
    "`author_id    hour`      \n",
    "\n",
    "For example:    \n",
    "\n",
    "```\n",
    "13431511\\t13\n",
    "54525254141\\t21\n",
    "```  \n",
    "\n",
    "If there is a tie: there are multiple hours during which a student has posted a maximum number of posts, please print the student-hour pairs on separate lines. The order in which these lines appear in your output does not matter.      \n",
    "\n",
    "You can ignore the time-zone offset for all times - for example in the following line: \"2012-02-25 08:11:01.623548+00\" - you can ignore the +00 offset.     \n",
    "\n",
    "In order to find the hour posted, please use the date_added field and NOT the last_activity_at field.       \n",
    "\n",
    "To make sure your code is running properly, we have put together a smaller data set and set of expected outputs for you to use to check your work. Please click [here](https://www.udacity.com/wiki/ud617/local-testing-instructions \"https://www.udacity.com/wiki/ud617/local-testing-instructions\") to access the instructions to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_studentTime.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_studentTime.py\n",
    "#!/usr/bin/python \n",
    "#Author: Zach Farmer\n",
    "#Purpose: find each students most active posting hour.  \n",
    "\n",
    "import sys\n",
    "import csv \n",
    "from datetime import datetime  \n",
    "\n",
    "def mapper():\n",
    "    \"\"\"\n",
    "        parameters: sys.stdin from Udacity forum_node data\n",
    "        \n",
    "        Output: key,value pair where key is the author_id and \n",
    "        the value is the added_at hour \n",
    "    \"\"\"\n",
    "    \n",
    "    reader = csv.reader(sys.stdin, delimiter=\"\\t\") \n",
    "    writer = csv.writer(sys.stdout, delimiter=\"\\t\", quotechar = '\"',\\\n",
    "                        quoting=csv.QUOTE_ALL)  \n",
    "    \n",
    "    for line in reader: \n",
    "        \n",
    "        \n",
    "        author_id = line[3] \n",
    "        #                     %Y    %m     %d   %H    %M      %S      %f               %z                  \n",
    "        #datetime Attributes: year, month, day, hour, minute, second, microsecond, and tzinfo.\n",
    "        if line[8] != \"added_at\": #ignore header row\n",
    "            time = line[8].split(\"+\")[0] #ignore timezone offset \n",
    "            hour = datetime.strptime(time, \"%Y-%m-%d %H:%M:%S.%f\").hour\n",
    "            writer.writerow([author_id,hour])\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    mapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"100000005\"\t\"1\"\r\n",
      "\"100000066\"\t\"1\"\r\n",
      "\"100000066\"\t\"5\"\r\n",
      "\"100002460\"\t\"12\"\r\n",
      "\"100003192\"\t\"8\"\r\n",
      "\"100003268\"\t\"15\"\r\n",
      "\"100004467\"\t\"12\"\r\n",
      "\"100004467\"\t\"20\"\r\n",
      "\"100004819\"\t\"10\"\r\n",
      "\"100004819\"\t\"4\"\r\n",
      "\"100004819\"\t\"4\"\r\n",
      "\"100004819\"\t\"5\"\r\n",
      "\"100005156\"\t\"17\"\r\n",
      "\"100007808\"\t\"12\"\r\n",
      "\"100008254\"\t\"22\"\r\n",
      "\"100010128\"\t\"14\"\r\n",
      "\"100012200\"\t\"5\"\r\n",
      "\"100019875\"\t\"5\"\r\n",
      "\"100020526\"\t\"14\"\r\n",
      "\"100071170\"\t\"12\"\r\n",
      "\"100071170\"\t\"12\"\r\n",
      "\"100071170\"\t\"14\"\r\n",
      "\"100071170\"\t\"5\"\r\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "##Test mapper output\n",
    "#chmod 764 mapper_studentTime.py #make executable \n",
    "cat student_test_posts.csv | ./mapper_studentTime.py | sort "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_studentTime_sansNumpy.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_studentTime_sansNumpy.py   \n",
    "#!/usr/bin/python   \n",
    "#Author: Zach \n",
    "#Purpose: find each students most active posting hour. This script does not \n",
    "#utilize numpy. \n",
    "\n",
    "import sys\n",
    "import csv \n",
    "#import numpy as np\n",
    "\n",
    "def reducer():\n",
    "    \"\"\"\n",
    "        paramters: sys.stdin from mapper_studentTime \n",
    "        \n",
    "        Output: Author_id followed by the hour were the id was most\n",
    "        active.   \n",
    "    \"\"\"\n",
    "    reader = csv.reader(sys.stdin, delimiter=\"\\t\")   \n",
    "    writer = csv.writer(sys.stdout, delimiter=\"\\t\", quotechar = '\"',\\\n",
    "                        quoting=csv.QUOTE_ALL)  \n",
    "    \n",
    "    oldKey = None\n",
    "    hours = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    \n",
    "    for line in reader:\n",
    "        currentKey = line[0]  \n",
    "        currentHour = line[1]\n",
    "        \n",
    "        if oldKey and oldKey != currentKey:\n",
    "            \n",
    "            most_active_hours = [(hour_count,idx) for idx,hour_count in enumerate(hours)]\n",
    "            most_active_hours.sort(key = lambda x: x[0], reverse=True) \n",
    "            \n",
    "            i = 0 \n",
    "            while sorted(hours,reverse=True)[0] == most_active_hours[i][0]:\n",
    "                #print \"{0}\\t{1}\".format(oldKey,most_active_hours[i][1])\n",
    "                writer.writerow([oldKey,most_active_hours[i][1]])\n",
    "                i += 1\n",
    "            oldKey = currentKey\n",
    "            hours = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "            \n",
    "        oldKey = currentKey\n",
    "        hours[int(currentHour.replace('\"',''))] += 1\n",
    "\n",
    "\n",
    "    active_hours = [(hour,count) for count,hour in enumerate(hours)]\n",
    "    active_hours.sort(key = lambda x: x[0], reverse= True)\n",
    "    i = 0\n",
    "    while sorted(hours,reverse=True)[0] == active_hours[i][0]:\n",
    "        #print \"{0}\\t{1}\".format(oldKey,active_hours[i][1])\n",
    "        writer.writerow([oldKey,active_hours[i][1]])\n",
    "        i += 1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    reducer() \n",
    "              \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_studentTime.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_studentTime.py   \n",
    "#!/usr/bin/python   \n",
    "#Author: Zach \n",
    "#Purpose: find each students most active posting hour. This script utilizes the \n",
    "#numpy module. I personally prefer this method but we cannot assume that all the \n",
    "#machines in the HDFS cluster will contain a numpy distribution. \n",
    "\n",
    "import sys\n",
    "import csv \n",
    "import numpy as np\n",
    "\n",
    "def reducer():\n",
    "    \"\"\"\n",
    "        paramters: sys.stdin from mapper_studentTime \n",
    "        \n",
    "        Output: Author_id followed by the hour were the id was most\n",
    "        active.   \n",
    "    \"\"\"\n",
    "    reader = csv.reader(sys.stdin, delimiter=\"\\t\")   \n",
    "    writer = csv.writer(sys.stdout, delimiter=\"\\t\", quotechar = '\"',\\\n",
    "                        quoting=csv.QUOTE_ALL)  \n",
    "    \n",
    "    oldKey = None\n",
    "    hours = np.zeros(24)\n",
    "    \n",
    "    for line in reader:\n",
    "        currentKey = line[0]  \n",
    "        currentHour = line[1]\n",
    "        \n",
    "        if oldKey and oldKey != currentKey:\n",
    "            \n",
    "            most_active_hours = np.argsort(-hours)\n",
    "            \n",
    "            i = 0 \n",
    "            while hours[np.argmax(hours)] == hours[most_active_hours[i]]:\n",
    "                #print \"{0}\\t{1}\".format(oldKey, most_active_hours[i])\n",
    "                writer.writerow([oldKey,most_active_hours[i]])\n",
    "                i += 1\n",
    "            oldKey = currentKey\n",
    "            hours = np.zeros(24)\n",
    "    \n",
    "        oldKey = currentKey\n",
    "        hours[int(currentHour.replace('\"',''))] += 1\n",
    "\n",
    "\n",
    "    most_active_hours = np.argsort(-hours)\n",
    "    i = 0 \n",
    "    while hours[np.argmax(hours)] == hours[most_active_hours[i]]:\n",
    "        #print \"{0}\\t{1}\".format(oldKey, most_active_hours[i])\n",
    "        writer.writerow([oldKey,most_active_hours[i]])\n",
    "        i += 1\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    reducer() \n",
    "              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"100000005\"\t\"1\"\r\n",
      "\"100000066\"\t\"1\"\r\n",
      "\"100000066\"\t\"5\"\r\n",
      "\"100002460\"\t\"12\"\r\n",
      "\"100003192\"\t\"8\"\r\n",
      "\"100003268\"\t\"15\"\r\n",
      "\"100004467\"\t\"12\"\r\n",
      "\"100004467\"\t\"20\"\r\n",
      "\"100004819\"\t\"4\"\r\n",
      "\"100005156\"\t\"17\"\r\n",
      "\"100007808\"\t\"12\"\r\n",
      "\"100008254\"\t\"22\"\r\n",
      "\"100010128\"\t\"14\"\r\n",
      "\"100012200\"\t\"5\"\r\n",
      "\"100019875\"\t\"5\"\r\n",
      "\"100020526\"\t\"14\"\r\n",
      "\"100071170\"\t\"12\"\r\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "##Test mapper output\n",
    "#chmod 764 reducer_studentTime.py #make executable \n",
    "#chmod 764 reducer_studentTime_sansNumpy.py\n",
    "cat student_test_posts.csv | ./mapper_studentTime.py | sort | ./reducer_studentTime_sansNumpy.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run on the whole dataset\n",
    "#!cat forum_data/forum_node.tsv | ./mapper_studentTime.py | sort | ./reducer_studentTime_sansNumpy.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Post and Answer Length\"></a>  \n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Post and Answer Length     \n",
    "We are interested to see if there is a correlation between the length of a post and the length of answers.   \n",
    "\n",
    "Write a mapreduce program that would process the forum_node data and output the length of the post and the average answer (just answer, not comment) length for each post. You will have to decide how to write both the mapper and the reducer to get the required result.\n",
    "\n",
    "To make sure your code is running properly, we have put together a smaller data set and set of expected outputs for you to use to check your work. Please click [here](https://www.udacity.com/wiki/ud617/local-testing-instructions) to access the instructions to use it.  \n",
    "\n",
    "> Hints for writing reducer code         \n",
    "Code should not use a data structure (e.g. a dictionary) in the reducer that stores a large number of keys. Remember that Hadoop already sorts the mapper output based on key, such that key-value pairs with the same key will appear consecutively as input to the reducer. Make sure you take advantage of this ordering when you write your reducer code.           \n",
    "This is part of a more general principle connected with the Volume characteristic of Big Data. Mappers and reducers read through very large amounts of data and we should be mindful, as we write mapper and reducer code, of how much data we store in main memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_PostLength.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_PostLength.py   \n",
    "#!/usr/bin/python \n",
    "#Author: Zach Farmer\n",
    "#Purpose: determine if correlation btw. post length and answers length.   \n",
    "\n",
    "import sys\n",
    "import csv \n",
    "\n",
    "def mapper():\n",
    "    \"\"\"\n",
    "        paramters: sys.stdin from forum_node.tsv or small test dataset \n",
    "        \n",
    "        Output: Either node_id if node_type question or abs_parent_id if\n",
    "        node_type answer as keys and the length of the body for values.\n",
    "    \"\"\"\n",
    "    \n",
    "    reader = csv.reader(sys.stdin, delimiter=\"\\t\")   \n",
    "    writer = csv.writer(sys.stdout, delimiter=\"\\t\", quotechar = '\"',\\\n",
    "                        quoting=csv.QUOTE_ALL)  \n",
    "    \n",
    "    for line in reader:\n",
    "        \n",
    "        node_type = line[5] \n",
    "        body_len = len(line[4]) \n",
    "        \n",
    "        if node_type == \"question\":\n",
    "            writer.writerow([line[0],\"Q-\"+str(body_len)])\n",
    "        elif node_type == \"answer\":\n",
    "            writer.writerow([line[7],\"A-\"+str(body_len)])\n",
    "    \n",
    "        \n",
    "if __name__==\"__main__\":\n",
    "    mapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"111\"\t\"Q-35\"\r\n",
      "\"15084\"\t\"Q-237\"\r\n",
      "\"2\"\t\"Q-145\"\r\n",
      "\"262\"\t\"Q-50\"\r\n",
      "\"26454\"\t\"Q-101\"\r\n",
      "\"3778\"\t\"A-164\"\r\n",
      "\"3778\"\t\"Q-69\"\r\n",
      "\"6011204\"\t\"A-158\"\r\n",
      "\"6011204\"\t\"A-219\"\r\n",
      "\"6011204\"\t\"Q-2651\"\r\n",
      "\"6011936\"\t\"A-125\"\r\n",
      "\"6011936\"\t\"A-760\"\r\n",
      "\"6011936\"\t\"Q-347\"\r\n",
      "\"6012754\"\t\"A-414\"\r\n",
      "\"6012754\"\t\"Q-369\"\r\n",
      "\"6015491\"\t\"A-313\"\r\n",
      "\"6015491\"\t\"A-65\"\r\n",
      "\"6015491\"\t\"Q-170\"\r\n",
      "\"66193\"\t\"A-288\"\r\n",
      "\"66193\"\t\"A-302\"\r\n",
      "\"66193\"\t\"A-34\"\r\n",
      "\"66193\"\t\"Q-60\"\r\n",
      "\"7185\"\t\"Q-86\"\r\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "#chmod 764 mapper_PostLength.py\n",
    "cat student_test_posts.csv | ./mapper_PostLength.py |sort "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_PostLength.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_PostLength.py\n",
    "#!/usr/bin/python  \n",
    "#Author: Zach Farmer\n",
    "#Purpose: Determine if correlation btw. post length and answers length.   \n",
    "\n",
    "import sys\n",
    "import csv \n",
    "\n",
    "def reducer(): \n",
    "    \"\"\"\n",
    "        paramters: sys.stdin from mapper_PostLength \n",
    "        \n",
    "        Output: Post length of question and average post lenght of answers\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    reader = csv.reader(sys.stdin, delimiter=\"\\t\")   \n",
    "    writer = csv.writer(sys.stdout, delimiter=\"\\t\", quotechar = '\"',\\\n",
    "                        quoting=csv.QUOTE_ALL)  \n",
    "    \n",
    "    oldKey = None\n",
    "    q_post_len = 0.0\n",
    "    answ_len = 0.0 \n",
    "    count = 0\n",
    "    \n",
    "    for line in reader: \n",
    "        \n",
    "        currentKey = line[0]\n",
    "        node_type,post_length = line[1].split(\"-\") \n",
    "        \n",
    "        if oldKey and oldKey != currentKey:\n",
    "            if count != 0:\n",
    "                avg_answ_len = answ_len/float(count)\n",
    "            else:\n",
    "                avg_answ_len = 0\n",
    "            writer.writerow([oldKey, q_post_len, avg_answ_len])\n",
    "            oldKey = currentKey\n",
    "            answ_len = 0.0\n",
    "            count = 0\n",
    "        \n",
    "        oldKey = currentKey\n",
    "        \n",
    "        if node_type == \"Q\":\n",
    "            q_post_len = int(post_length.replace('\"',''))\n",
    "        elif node_type == \"A\":\n",
    "            answ_len += int(post_length.replace('\"',''))\n",
    "            count += 1\n",
    "    \n",
    "    if count != 0:\n",
    "        avg_answ_len = answ_len/float(count)\n",
    "    else:\n",
    "        avg_answ_len = 0\n",
    "    writer.writerow([oldKey,q_post_len, avg_answ_len])\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    reducer()   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"111\"\t\"35\"\t\"0\"\r\n",
      "\"15084\"\t\"237\"\t\"0\"\r\n",
      "\"2\"\t\"145\"\t\"0\"\r\n",
      "\"262\"\t\"50\"\t\"0\"\r\n",
      "\"26454\"\t\"101\"\t\"0\"\r\n",
      "\"3778\"\t\"69\"\t\"164.0\"\r\n",
      "\"6011204\"\t\"2651\"\t\"188.5\"\r\n",
      "\"6011936\"\t\"347\"\t\"442.5\"\r\n",
      "\"6012754\"\t\"369\"\t\"414.0\"\r\n",
      "\"6015491\"\t\"170\"\t\"189.0\"\r\n",
      "\"66193\"\t\"60\"\t\"208.0\"\r\n",
      "\"7185\"\t\"86\"\t\"0\"\r\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "#chmod 764 reducer_PostLength.py\n",
    "cat student_test_posts.csv | ./mapper_PostLength.py |sort |./reducer_PostLength.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run on the whole dataset\n",
    "#!cat forum_data/forum_node.tsv | ./mapper_PostLength.py |sort |./reducer_PostLength.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Top Tags\"></a>    \n",
    "**** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Top Tags    \n",
    "We are interested seeing what are the top tags used in posts.    \n",
    "\n",
    "Write a mapreduce program that would output Top 10 tags, ordered by the number of questions they appear in.\n",
    "\n",
    "For an extra challenge you can think about how to get a top 10 list of tags, where they are ordered by some weighted score of your choice. If you decide to do this, then please submit your solution to the regular problem and then also submit this extra challenge problem in separate files as described on the instruction page.\n",
    "\n",
    "To make sure your code is running properly, we have put together a smaller data set and set of expected outputs for you to use to check your work. Please click [here](https://www.udacity.com/wiki/ud617/local-testing-instructions) to access the instructions to use it.\n",
    "\n",
    "Please note that you should only look at tags appearing in questions themselves (i.e. nodes with node_type \"question\"), not on answers or comments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_TopTags.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_TopTags.py   \n",
    "#!/usr/bin/python  \n",
    "#Author: Zach Farmer\n",
    "#Purpose: Return Top 10 tags found in questions \n",
    "\n",
    "import sys\n",
    "import csv    \n",
    "\n",
    "def mapper():   \n",
    "    \"\"\"\n",
    "        Parameter: sys.stdin from forum_node.tsv or test file.\n",
    "        \n",
    "        Output: tab deliniated csv lines for input to reducer_TopTags.py\n",
    "    \"\"\"\n",
    "    \n",
    "    reader = csv.reader(sys.stdin, delimiter=\"\\t\")   \n",
    "    writer = csv.writer(sys.stdout, delimiter=\"\\t\", quotechar = '\"',\\\n",
    "                        quoting=csv.QUOTE_ALL)  \n",
    "    \n",
    "    for line in reader:\n",
    "        \n",
    "        node_id = line[0]\n",
    "        node_type = line[5]     \n",
    "        tags = line[2].split(\" \")      \n",
    "        \n",
    "        if node_type == \"question\":\n",
    "            for tag in tags:\n",
    "                #print \"{0}\\t{1}\".format(line[0],tag)\n",
    "                writer.writerow([tag,line[0]])\n",
    "            \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    mapper() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"application\"\t\"26454\"\r\n",
      "\"board\"\t\"15084\"\r\n",
      "\"browsers\"\t\"262\"\r\n",
      "\"bug\"\t\"111\"\r\n",
      "\"cs101\"\t\"111\"\r\n",
      "\"cs101\"\t\"15084\"\r\n",
      "\"cs101\"\t\"2\"\r\n",
      "\"cs101\"\t\"262\"\r\n",
      "\"cs101\"\t\"26454\"\r\n",
      "\"cs101\"\t\"3778\"\r\n",
      "\"cs101\"\t\"66193\"\r\n",
      "\"cs101\"\t\"7185\"\r\n",
      "\"cs212\"\t\"66193\"\r\n",
      "\"cs253\"\t\"6011204\"\r\n",
      "\"cs253\"\t\"6011936\"\r\n",
      "\"cs253\"\t\"6012754\"\r\n",
      "\"cs253\"\t\"6015491\"\r\n",
      "\"cs253\"\t\"66193\"\r\n",
      "\"cs262\"\t\"66193\"\r\n",
      "\"deadlines\"\t\"6012754\"\r\n",
      "\"digital\"\t\"15084\"\r\n",
      "\"discussion\"\t\"26454\"\r\n",
      "\"discussion\"\t\"6011204\"\r\n",
      "\"discussion\"\t\"6011936\"\r\n",
      "\"discussion\"\t\"6015491\"\r\n",
      "\"discussion\"\t\"66193\"\r\n",
      "\"google-appengine\"\t\"6011936\"\r\n",
      "\"homework\"\t\"6011204\"\r\n",
      "\"homework\"\t\"6012754\"\r\n",
      "\"html\"\t\"6011936\"\r\n",
      "\"hungarian\"\t\"7185\"\r\n",
      "\"hw2-1\"\t\"6012754\"\r\n",
      "\"issues\"\t\"111\"\r\n",
      "\"issues\"\t\"262\"\r\n",
      "\"issues\"\t\"3778\"\r\n",
      "\"jobs\"\t\"15084\"\r\n",
      "\"jobs\"\t\"26454\"\r\n",
      "\"lessons\"\t\"15084\"\r\n",
      "\"lessons\"\t\"66193\"\r\n",
      "\"meta\"\t\"111\"\r\n",
      "\"meta\"\t\"66193\"\r\n",
      "\"nationalities\"\t\"111\"\r\n",
      "\"nationalities\"\t\"7185\"\r\n",
      "\"offtopic\"\t\"6015491\"\r\n",
      "\"profile\"\t\"3778\"\r\n",
      "\"udacity\"\t\"6015491\"\r\n",
      "\"udacity-future\"\t\"6015491\"\r\n",
      "\"video\"\t\"111\"\r\n",
      "\"welcome\"\t\"2\"\r\n",
      "\"welcome\"\t\"66193\"\r\n",
      "\"welcome\"\t\"7185\"\r\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "chmod 764 mapper_TopTags.py   \n",
    "cat student_test_posts.csv | ./mapper_TopTags.py | sort "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_TopTags.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_TopTags.py\n",
    "#!/usr/bin/python  \n",
    "#Author: Zach Farmer\n",
    "#Purpose: Return Top 10 tags found in questions   \n",
    "\n",
    "import sys\n",
    "import csv \n",
    "#from collections import defaultdict \n",
    "\n",
    "def reducer(): \n",
    "    \"\"\"\n",
    "        paramters: sys.stdin from mapper_TopTags.py\n",
    "        \n",
    "        Output: Top ten tags from question nodes \n",
    "    \"\"\"\n",
    "    \n",
    "    reader = csv.reader(sys.stdin, delimiter=\"\\t\")   \n",
    "    writer = csv.writer(sys.stdout, delimiter=\"\\t\", quotechar = '\"',\\\n",
    "                        quoting=csv.QUOTE_ALL)  \n",
    "    \n",
    "    oldKey = None \n",
    "    count = 0 \n",
    "    \n",
    "    #tag_dict = defaultdict(int)\n",
    "    topTen_tagDict = {}\n",
    "    for line in reader:\n",
    "    \n",
    "        currentKey = line[0]   \n",
    "        \n",
    "        if oldKey and oldKey != currentKey:\n",
    "            if len(topTen_tagDict.keys()) < 10:\n",
    "                topTen_tagDict[oldKey] = count\n",
    "                oldKey = currentKey\n",
    "                count = 0 \n",
    "            elif len(topTen_tagDict.keys()) >= 10:\n",
    "                if count > min(topTen_tagDict.values()):\n",
    "                    del topTen_tagDict[[k for k, v in topTen_tagDict.iteritems()\\\n",
    "                                         if v == min(topTen_tagDict.values())][0]]\n",
    "                    topTen_tagDict[oldKey] = count \n",
    "                    oldKey = currentKey \n",
    "                    count = 0 \n",
    "                else:\n",
    "                    oldKey = currentKey\n",
    "                    count = 0 \n",
    "            #print \"{0}\\t{1}\".format(count,oldKey)\n",
    "            #count = 0\n",
    "        \n",
    "        oldKey = currentKey \n",
    "        count += 1\n",
    "    \n",
    "    #Method of returning dict key from value courtesy of Chris Morgan at:\n",
    "    #http://stackoverflow.com/questions/7657457/finding-key-from-value-in-python-dictionary\n",
    "    if count > min(topTen_tagDict.values()):\n",
    "        del topTen_tagDict[[k for k, v in topTen_tagDict.iteritems()\\\n",
    "                            if v == min(topTen_tagDict.values())][0]]\n",
    "        topTen_tagDict[oldKey] = count   \n",
    "        \n",
    "    for key,count in sorted(topTen_tagDict.items(), key = lambda x: x[1], reverse=True):\n",
    "        writer.writerow([key,count]) \n",
    "        \n",
    "    #print \"{0}\\t{1}\".format(count,oldKey)\n",
    "        #tag_dict[currentKey] += 1\n",
    "        \n",
    "    #for key,count in sorted(tag_dict.items(), key = lambda x: x[1],reverse=True)[0:10]:\n",
    "        #print \"{0}\\t{1}\".format(key,count)\n",
    "        #writer.writerow([count,key]) \n",
    "            \n",
    "if __name__ == \"__main__\": \n",
    "    reducer()  \n",
    "    \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"cs101\"\t\"8\"\r\n",
      "\"discussion\"\t\"5\"\r\n",
      "\"cs253\"\t\"5\"\r\n",
      "\"welcome\"\t\"3\"\r\n",
      "\"issues\"\t\"3\"\r\n",
      "\"lessons\"\t\"2\"\r\n",
      "\"jobs\"\t\"2\"\r\n",
      "\"meta\"\t\"2\"\r\n",
      "\"nationalities\"\t\"2\"\r\n",
      "\"homework\"\t\"2\"\r\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "#chmod 764 reducer_TopTags.py \n",
    "cat student_test_posts.csv | ./mapper_TopTags.py | sort | ./reducer_TopTags.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run on the whole dataset\n",
    "#!cat forum_data/forum_node.tsv | ./mapper_TopTags.py | sort | ./reducer_TopTags.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Study Groups\"></a>   \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Study Groups    \n",
    "We might want to help students form study groups. But first we want to see if there are already students on forums that communicate a lot between themselves.   \n",
    "\n",
    "As the first step for this analysis we have been tasked with writing a mapreduce program that for each forum thread (that is a question node with all it's answers and comments) would give us a list of students that have posted there - either asked the question, answered a question or added a comment. If a student posted to that thread several times, they should be added to that list several times as well, to indicate intensity of communication.     \n",
    "\n",
    "To make sure your code is running properly, we have put together a smaller data set and set of expected outputs for you to use to check your work. Please click [here](https://www.udacity.com/wiki/ud617/local-testing-instructions) to access the instructions to use it.     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_studyGroups.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_studyGroups.py   \n",
    "#!/usr/bin/python  \n",
    "#Author: Zach Farmer  \n",
    "#Purpose: Find potential study groups by measuring frequency of students post per question.  \n",
    "\n",
    "import sys \n",
    "import csv   \n",
    "\n",
    "def mapper():\n",
    "    \"\"\"\n",
    "        parameter: sys.stdin from fourm_node.tsv or the sample test file student_tests_post.tsv\n",
    "        \n",
    "        output: sys.stdout .tsv lines containing the post node_id and the author_ids \n",
    "        of the question node and each child answer and comment post. Will include duplicate \n",
    "        author_id's if author posted more then once.\n",
    "    \"\"\"\n",
    "    \n",
    "    reader = csv.reader(sys.stdin, delimiter=\"\\t\")\n",
    "    writer = csv.writer(sys.stdout, delimiter=\"\\t\", quotechar='\"',\\\n",
    "                        quoting = csv.QUOTE_ALL)   \n",
    "    \n",
    "    for line in reader:\n",
    "        \n",
    "        node_id = line[0]\n",
    "        node_type = line[5]  \n",
    "        author_id = line[3] \n",
    "        abs_parent_id = line[7]\n",
    "        \n",
    "        \n",
    "        if node_type == \"question\":\n",
    "            writer.writerow([node_id,author_id])\n",
    "        elif node_type == \"answer\" or node_type == \"comment\":\n",
    "            writer.writerow([abs_parent_id, author_id])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mapper()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"111\"\t\"100000066\"\r\n",
      "\"15084\"\t\"100004819\"\r\n",
      "\"2\"\t\"100000005\"\r\n",
      "\"262\"\t\"100004819\"\r\n",
      "\"26454\"\t\"100003192\"\r\n",
      "\"3778\"\t\"100000066\"\r\n",
      "\"3778\"\t\"100008254\"\r\n",
      "\"6011204\"\t\"100010128\"\r\n",
      "\"6011204\"\t\"100020526\"\r\n",
      "\"6011204\"\t\"100071170\"\r\n",
      "\"6011936\"\t\"100004819\"\r\n",
      "\"6011936\"\t\"100019875\"\r\n",
      "\"6011936\"\t\"100071170\"\r\n",
      "\"6012754\"\t\"100004819\"\r\n",
      "\"6012754\"\t\"100012200\"\r\n",
      "\"6015491\"\t\"100004467\"\r\n",
      "\"6015491\"\t\"100005156\"\r\n",
      "\"6015491\"\t\"100071170\"\r\n",
      "\"66193\"\t\"100002460\"\r\n",
      "\"66193\"\t\"100004467\"\r\n",
      "\"66193\"\t\"100007808\"\r\n",
      "\"66193\"\t\"100071170\"\r\n",
      "\"7185\"\t\"100003268\"\r\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "#chmod 764 mapper_studyGroups.py \n",
    "cat student_test_posts.csv | ./mapper_studyGroups.py | sort "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_studyGroups.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_studyGroups.py   \n",
    "#!/usr/bin/python  \n",
    "#Author: Zach Farmer  \n",
    "#Purpose: Find potential study groups by measuring frequency of students post per question.  \n",
    "\n",
    "import sys \n",
    "import csv  \n",
    "\n",
    "def reducer(): \n",
    "    \"\"\"\n",
    "        parameter: sys.stdin from mapper_studyGroups.py\n",
    "        \n",
    "        output: sys.stdout .tsv lines containing the post node_id for the top level\n",
    "        question and all the author_ids associated with post related to the question.\n",
    "    \"\"\"\n",
    "    \n",
    "    reader = csv.reader(sys.stdin, delimiter=\"\\t\")\n",
    "    writer = csv.writer(sys.stdout, delimiter=\"\\t\", quotechar='\"',\\\n",
    "                        )   \n",
    "    \n",
    "    oldKey = None\n",
    "    author_ids = []\n",
    "    for line in reader:\n",
    "        \n",
    "        currentKey = line[0] \n",
    "        \n",
    "        if oldKey and oldKey != currentKey: \n",
    "            writer.writerow([oldKey,author_ids])\n",
    "            oldKey = currentKey\n",
    "            author_ids = []\n",
    "            \n",
    "        oldKey = currentKey \n",
    "        author_ids.append(int(line[1]))\n",
    "            \n",
    "    writer.writerow([oldKey,author_ids])\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    reducer()  \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\t[100000066]\r\n",
      "15084\t[100004819]\r\n",
      "2\t[100000005]\r\n",
      "262\t[100004819]\r\n",
      "26454\t[100003192]\r\n",
      "3778\t[100000066, 100008254]\r\n",
      "6011204\t[100010128, 100020526, 100071170]\r\n",
      "6011936\t[100004819, 100019875, 100071170]\r\n",
      "6012754\t[100004819, 100012200]\r\n",
      "6015491\t[100004467, 100005156, 100071170]\r\n",
      "66193\t[100002460, 100004467, 100007808, 100071170]\r\n",
      "7185\t[100003268]\r\n"
     ]
    }
   ],
   "source": [
    "%%bash  \n",
    "#chmod 764 reducer_studyGroups.py  \n",
    "cat student_test_posts.csv | ./mapper_studyGroups.py | sort | ./reducer_studyGroups.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run on the whole dataset\n",
    "#!cat forum_data/forum_node.tsv | ./mapper_studyGroups.py | sort | ./reducer_studyGroups.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Search Functionality\"></a>   \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Search Functionality      \n",
    "**Improving the search functionality and index-building**    \n",
    "In lesson 4 you built an index which included {\"word\":\"forum entries that include the word\"}. This can be used to search efficiently for forum posts that contain a specific word. Can you think of improvements you could make to the process of building an index by using the design patterns you learned in Lesson 4?   \n",
    "\n",
    "The improvements might include improving the efficiency of the index building by applying some of the MapReduce design patterns or changing the index to include other features from the data.    \n",
    "\n",
    "**Answer:**    \n",
    "If we were to stick to the same index-building features I would consider implementing a combiner in order to sub-aggregate the words and their node_ids in each of the mappers machines. This would reduce the quantity of traffic passed to the reduce and ideally speed up the entire map-reduce process. If however we were considering changing the methodology a little bit I might consider a.) removing stop words and b.) only return the parent question node_id in which the word is nested in. This would require more searching on the part of the individual using the index but greatly reduce the map-reduce task. More radically I might also consider just indexing the tags of the nodes, as most people I imagine use an index to search for a particular topics in order to find post on the subject. Tags ideally should summarize the key topic discussed in the post. On the topic of relevancy we could order the node_id lists by the percieved value of the word in a post. For example if the word were to be found many times in a post then that posts node_id would be given higher priority and listed closer to the begining of listed nodes. We might also consider more heavily weighting nodes were the relevant word can be found in the tag or title of the post, giving a node_id higher preference in listing order on the basis that their information may be more relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
